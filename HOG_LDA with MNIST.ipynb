{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đồ án thị giác máy tính\n",
    "* Sử dụng đặc trưng HOG trong bài báo \"An improved handwritten Chinese character recognition system using support vector machine\" để rút trích vector đặc trưng trên tập MNIST\n",
    "* Giảm số chiều vector đặc trưng bằng LDA\n",
    "* Phân lớp dựa trên khoảng cách Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thêm các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import ndimage\n",
    "import gzip\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import math\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "NORMAL_SAMPLE_PICTURE_LENGTH = 64\n",
    "BLOCK_NUMBER_PIXEL_PER_DIMENSION = 8\n",
    "PIXEL_STEP_PER_BLOCK = 8\n",
    "LENGTH_OF_NORMALIZE_PICTURE = 80\n",
    "SIZE_OF_FEATURE_BLOCK = 16\n",
    "\n",
    "NUMBER_OF_BLOCK_PER_ROW = (LENGTH_OF_NORMALIZE_PICTURE - PIXEL_STEP_PER_BLOCK) / PIXEL_STEP_PER_BLOCK #equal 9 \n",
    "\n",
    "SIZE_OF_FEATURE_VECTOR_ONE_BLOCK = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm đọc file dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    \"\"\"\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.pkl.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (train_X, train_Y, val_X, val_Y, test_X, test_Y) : tuple\n",
    "        train_X : numpy array, shape (N=50000, d=784)\n",
    "            Input vectors of the training set.\n",
    "        train_Y: numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        val_X : numpy array, shape (N=10000, d=784)\n",
    "            Input vectors of the validation set.\n",
    "        val_Y: numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        test_X : numpy array, shape (N=10000, d=784)\n",
    "            Input vectors of the test set.\n",
    "        test_Y: numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    \"\"\"\n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    \n",
    "    train_X, train_Y = train_data\n",
    "    val_X, val_Y = val_data\n",
    "    test_X, test_Y = test_data    \n",
    "\n",
    "    return train_X, train_Y, val_X, val_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm tìm vector đặc trừng từ tập dữ liệu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm tính khu vực A: **Grad_A(rowIndex, colIndex, gradientMagnitude)**  \n",
    "Hàm tính khu vực B: **Grad_B(rowIndex, colIndex, gradientMagnitude)**  \n",
    "Hàm tính khu vực C: **Grad_C(rowIndex, colIndex, gradientMagnitude)**  \n",
    "Hàm tính khu vực D: **Grad_D(rowIndex, colIndex, gradientMagnitude)**  \n",
    "Hàm tính vector 16 chiều:  **Extract_Feature_From_A_Block**\n",
    "Hàm rút trích đặc trưng HOG: **HOG_Feature_Extractor(rawData)**  \n",
    "Hàm tìm giá trị tối đa trong vector đặc trưng (dùng để normalize): **Find_Feature_Max_Value(featureVector)**  \n",
    "Hàm chuẩn hóa vector: **Normalize_Feature_Vector(featureVector)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_A(rowIndex, colIndex, gradientMagnitude):\n",
    "    result = 0\n",
    "    row = (rowIndex + 6) * LENGTH_OF_NORMALIZE_PICTURE + 6 + colIndex\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3]\n",
    "    for i in range(0,2):\n",
    "        row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "        result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3]\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_B(rowIndex, colIndex, gradientMagnitude):\n",
    "    result = 0\n",
    "    row = (rowIndex + 4) * LENGTH_OF_NORMALIZE_PICTURE + 4 + colIndex\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7]\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7]\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "        result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 6] + gradientMagnitude[row + 7]\n",
    "\n",
    "\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7]\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_C(rowIndex, colIndex, gradientMagnitude):\n",
    "    result = 0\n",
    "    row = (rowIndex + 2) * LENGTH_OF_NORMALIZE_PICTURE + 2 + colIndex\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11]\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11]\n",
    "    \n",
    "    for i in range(0,7):\n",
    "        row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "        result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 10] + gradientMagnitude[row + 11]\n",
    "\n",
    "\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11]\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11]\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_D(rowIndex, colIndex, gradientMagnitude):\n",
    "    result = 0\n",
    "    row = rowIndex * LENGTH_OF_NORMALIZE_PICTURE + colIndex\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11] + gradientMagnitude[row + 12] + gradientMagnitude[row + 13] + gradientMagnitude[\n",
    "               row + 14] + gradientMagnitude[row + 15]\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11] + gradientMagnitude[row + 12] + gradientMagnitude[row + 13] + gradientMagnitude[\n",
    "               row + 14] + gradientMagnitude[row + 15]\n",
    "    \n",
    "    for i in range(0,11):\n",
    "        row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "        result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 14] + gradientMagnitude[row + 15]\n",
    "\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11] + gradientMagnitude[row + 12] + gradientMagnitude[row + 13] + gradientMagnitude[\n",
    "               row + 14] + gradientMagnitude[row + 15]\n",
    "    row += LENGTH_OF_NORMALIZE_PICTURE\n",
    "    result += gradientMagnitude[row] + gradientMagnitude[row + 1] + gradientMagnitude[row + 2] + gradientMagnitude[row + 3] + \\\n",
    "           gradientMagnitude[row + 4] + gradientMagnitude[row + 5] + gradientMagnitude[row + 6] + gradientMagnitude[\n",
    "               row + 7] + gradientMagnitude[row + 8] + gradientMagnitude[row + 9] + gradientMagnitude[row + 10] + \\\n",
    "           gradientMagnitude[row + 11] + gradientMagnitude[row + 12] + gradientMagnitude[row + 13] + gradientMagnitude[\n",
    "               row + 14] + gradientMagnitude[row + 15]\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractBlockFeatures(i , j, FeatureVector, gradientMagnitude, gradientAngles): \n",
    "    i = i * PIXEL_STEP_PER_BLOCK\n",
    "    MagnitudeFromAreaA = Grad_A(i, j, gradientMagnitude)\n",
    "    MagnitudeFromAreaB = Grad_B(i, j, gradientMagnitude)\n",
    "    MagnitudeFromAreaC = Grad_C(i, j, gradientMagnitude)\n",
    "    MagnitudeFromAreaD = Grad_D(i, j, gradientMagnitude)\n",
    "    magnitude = 4 * MagnitudeFromAreaA + 3 * MagnitudeFromAreaB + 2 * MagnitudeFromAreaC + MagnitudeFromAreaD\n",
    "    \n",
    "    stopRow = i + 16\n",
    "    stopCol = j + 16\n",
    "    kArray = np.zeros(32)\n",
    "\n",
    "    for row in range(i,stopRow-1):\n",
    "        for col in range(j, stopCol-1):\n",
    "            k = (gradientAngles[row * LENGTH_OF_NORMALIZE_PICTURE + col]) * 32 / 360 #converse angle to bin\n",
    "            angleBin = math.floor(k)\n",
    "\n",
    "            remain = (k - angleBin)\n",
    "            if angleBin > 31:\n",
    "                angleBin -= 32\n",
    "\n",
    "            kArray[angleBin] += magnitude * (1 - remain)\n",
    "            angleBin += 1\n",
    "            if angleBin > 31:\n",
    "                angleBin -= 32\n",
    "            kArray[angleBin] += magnitude * remain   \n",
    "    block = int((i * NUMBER_OF_BLOCK_PER_ROW + j) * SIZE_OF_FEATURE_VECTOR_ONE_BLOCK / PIXEL_STEP_PER_BLOCK)\n",
    "    temp = 6 * kArray[0] + 4 * kArray[1] + kArray[2]\n",
    "    FeatureVector[block] = float(math.pow(temp, 0.4))\n",
    "    block+=1\n",
    "    #print (block)\n",
    "    for k in range(2,28,2):\n",
    "        temp = kArray[k - 2] + 4 * kArray[k - 1] + 6 * kArray[k] + 4 * kArray[k + 1] + kArray[k + 2]\n",
    "        FeatureVector[block] = float(math.pow(temp, 0.4))\n",
    "        block += 1\n",
    "    temp = kArray[28] + 4 * kArray[29] + 6 * kArray[30] + 4 * kArray[31]\n",
    "    FeatureVector[block] = float(math.pow(temp, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG_Feature_Extractor(rawData):\n",
    "    \n",
    "    Data_HOG = np.zeros((rawData.shape[0],1296))\n",
    "    for indexOfData in range(0,Data_HOG.shape[0]-1):\n",
    "        img = rawData[indexOfData].reshape((28,28))\n",
    "        img = cv2.resize(img,(64,64))\n",
    "        \n",
    "        background = np.zeros((80,80))\n",
    "        \n",
    "        background[8:72,8:72] = img\n",
    "        \n",
    "        vertical_Robert = np.array( [[ 0, 0, 0 ],\n",
    "                                     [ 0, 1, 0 ],\n",
    "                                     [ 0, 0,-1 ]] )\n",
    "\n",
    "        horizontal_Robert = np.array( [[ 0, 0, 0 ],\n",
    "                                       [ 0, 0, 1 ],\n",
    "                                       [ 0,-1, 0 ]] )\n",
    "        gx = ndimage.convolve( background, vertical_Robert )\n",
    "        gy = ndimage.convolve( background, horizontal_Robert )\n",
    "        #Find gradient magnitude and orientation\n",
    "        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "        mag = mag.flatten()\n",
    "        angle = angle.flatten()\n",
    "        \n",
    "        for i in range(0, 9 , 1):\n",
    "            for j in range(0, LENGTH_OF_NORMALIZE_PICTURE - PIXEL_STEP_PER_BLOCK, PIXEL_STEP_PER_BLOCK):\n",
    "                ExtractBlockFeatures(i, j,Data_HOG[indexOfData], mag, angle)\n",
    "        Normalize_Feature_Vector(Data_HOG[indexOfData])\n",
    "    return Data_HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Feature_Max_Value(featureVector):\n",
    "    max = -float(\"inf\")\n",
    "    for i in range(len(featureVector)):\n",
    "        if(featureVector[i]>max):\n",
    "            max = featureVector[i]\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_Feature_Vector(featureVector):\n",
    "    max = Find_Feature_Max_Value(featureVector)\n",
    "    for i in range(len(featureVector)):\n",
    "        featureVector[i] = featureVector[i]/max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm giảm chiều dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Dimension_Reduction(trainData,label):\n",
    "    lda = LinearDiscriminantAnalysis(n_components = 100)\n",
    "    X_train_lda = lda.fit_transform(trainData, label)\n",
    "    return X_train_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm tìm mean vector của mỗi lớp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_Mean_Vectors(trainData, label):\n",
    "    meanMatrix = np.zeros((10,trainData.shape[1]))\n",
    "    numberVectorContribute = np.zeros(10)\n",
    "    for i in range(len(trainData)):\n",
    "        meanMatrix[int(label[i])] += trainData[i]\n",
    "        numberVectorContribute[int(label[i])] +=1\n",
    "    for i in range(10):\n",
    "        meanMatrix[i] = meanMatrix[i]/numberVectorContribute[i]\n",
    "    return meanMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm tính khoảng cách Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSquaredEuclideanDist(x,y):\n",
    "    return np.sum((x-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm phân lớp bằng Euclidean distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_Classifier(meanMatrix,testData):\n",
    "    mindist = GetSquaredEuclideanDist(testData,meanMatrix[0])\n",
    "    label = -1\n",
    "    for i in range(10):\n",
    "        if (GetSquaredEuclideanDist(testData,meanMatrix[i])<mindist):\n",
    "            mindist = GetSquaredEuclideanDist(testData,meanMatrix[i])\n",
    "            label = i\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chương trình chính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_X, train_Y, val_X, val_Y, test_X, test_Y = read_mnist('mnist.pkl.gz')\n",
    "    number_train = 5000\n",
    "    train_X_test = np.zeros((number_train,train_X.shape[1]))\n",
    "    train_Y_test = np.zeros(number_train)\n",
    "    \n",
    "    test_X_test = np.zeros((number_train,test_X.shape[1]))\n",
    "    test_Y_test = np.zeros(number_train)\n",
    "    for i in range(0,number_train -1):\n",
    "        train_Y_test[i] = train_Y[i]\n",
    "        train_X_test[i] = train_X[i]\n",
    "    for i in range(0,number_train -1):\n",
    "        test_Y_test[i] = test_Y[i]\n",
    "        test_X_test[i] = test_X[i]\n",
    "    average_Error_Rate = 0\n",
    "    \n",
    "    HOG_train = HOG_Feature_Extractor(train_X)\n",
    "    train_lda = Data_Dimension_Reduction(HOG_train,train_Y)\n",
    "    HOG_test = HOG_Feature_Extractor(test_X_test)\n",
    "    test_lda = Data_Dimension_Reduction(HOG_test,test_Y_test)\n",
    "    test_Data_Length = len(test_lda)\n",
    "    meanMatrix = Calculate_Mean_Vectors(train_lda,train_Y_test)\n",
    "    for i in range(test_Data_Length):\n",
    "        if(Euclidean_Classifier(meanMatrix,test_lda[i]) != test_Y_test[i]):\n",
    "            average_Error_Rate += 1/test_Data_Length\n",
    "    print(\"Average error rate: %f\" %average_Error_Rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-76c2cd9f6858>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mHOG_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHOG_Feature_Extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtrain_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_Dimension_Reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHOG_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mHOG_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHOG_Feature_Extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mtest_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_Dimension_Reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHOG_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_Y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8ec3d976cfa0>\u001b[0m in \u001b[0;36mData_Dimension_Reduction\u001b[1;34m(trainData, label)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mData_Dimension_Reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX_train_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train_lda\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinkage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shrinkage not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve_svd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lsqr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve_lsqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshrinkage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinkage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\discriminant_analysis.py\u001b[0m in \u001b[0;36m_solve_svd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxbar_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0mXc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;31m# 1) within (univariate) scaling by with classes std-dev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2():\n",
    "    train_X, train_Y, val_X, val_Y, test_X, test_Y = read_mnist('mnist.pkl.gz')\n",
    "    number_train = 3000\n",
    "    train_X_5000 = np.zeros((number_train,train_X.shape[1]))\n",
    "    train_Y_5000 = np.zeros(number_train)\n",
    "    k = 4\n",
    "    \n",
    "    test_X_5000 = np.zeros((number_train,test_X.shape[1]))\n",
    "    test_Y_5000 = np.zeros(number_train)\n",
    "    for i in range(0,number_train-1):\n",
    "        train_Y_5000[i] = train_Y[i]\n",
    "        train_X_5000[i] = train_X[i]\n",
    "    for i in range(0,number_train-1):\n",
    "        test_Y_5000[i] = test_Y[i]\n",
    "        test_X_5000[i] = test_X[i]\n",
    "    \n",
    "    HOG_train = HOG_Feature_Extractor(train_X_5000)\n",
    "    train_lda = Data_Dimension_Reduction(HOG_train,train_Y_5000)\n",
    "    HOG_test = HOG_Feature_Extractor(test_X_5000)\n",
    "    test_lda = Data_Dimension_Reduction(HOG_test,test_Y_5000)\n",
    "    test_Data_Length = len(test_lda)\n",
    "    meanMatrix = Calculate_Mean_Vectors(train_lda,train_Y_5000)\n",
    "    mindist = GetSquaredEuclideanDist(test_lda[k],meanMatrix[0])\n",
    "    label = 0\n",
    "    for i in range(0,10):\n",
    "        print(\"Khoang cach toi tap so %d la\" %i, GetSquaredEuclideanDist(test_lda[k],meanMatrix[i]))\n",
    "        if (GetSquaredEuclideanDist(test_lda[k],meanMatrix[i])<mindist):\n",
    "            mindist = GetSquaredEuclideanDist(test_lda[k],meanMatrix[i])\n",
    "            label = i\n",
    "    print(\"Khoang cach nho nhat la %f, thuoc so %d\" %(mindist,label) )\n",
    "    print(\"So chinh xac: %d\" %test_Y_5000[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
